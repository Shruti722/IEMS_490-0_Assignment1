{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e25966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:20.300977Z",
     "iopub.status.busy": "2025-10-21T01:12:20.300616Z",
     "iopub.status.idle": "2025-10-21T01:12:20.573099Z",
     "shell.execute_reply": "2025-10-21T01:12:20.572489Z"
    },
    "papermill": {
     "duration": 0.282301,
     "end_time": "2025-10-21T01:12:20.573495",
     "exception": false,
     "start_time": "2025-10-21T01:12:20.291194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client ready. MODEL_NAME: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "import os, time, random, hashlib, json, re\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "\n",
    "PROVIDER   = os.getenv(\"PROVIDER\", \"groq\").lower()\n",
    "BASE_URL   = os.getenv(\"BASE_URL\", \"https://api.groq.com/openai/v1\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3-8b-8192\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "assert PROVIDER == \"groq\", \"Set PROVIDER=groq in your .env\"\n",
    "assert GROQ_API_KEY, \"Missing GROQ_API_KEY in .env\"\n",
    "assert BASE_URL and MODEL_NAME, \"BASE_URL / MODEL_NAME missing\"\n",
    "\n",
    "_client = OpenAI(api_key=GROQ_API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# Retry/abort knobs\n",
    "class TooManyRateLimits(RuntimeError): ...\n",
    "MAX_RETRIES = 3\n",
    "PAUSE_ON_RATE_LIMIT_SEC = 5\n",
    "BASE_DELAY = 1.0\n",
    "BACKOFF_BASE = 1.6\n",
    "BACKOFF_CAP = 8.0\n",
    "CONSECUTIVE_429_ABORT = 3\n",
    "\n",
    "_llm_cache = {}\n",
    "def _hash_messages(messages, model, temperature, max_tokens):\n",
    "    payload = {\"m\": messages, \"model\": model, \"t\": temperature, \"max_tokens\": max_tokens}\n",
    "    return hashlib.sha256(json.dumps(payload, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "def chat_complete(messages: List[Dict], model: str = MODEL_NAME, temperature: float = 0.2,\n",
    "                  max_tokens: int = 512, timeout: int = 60, use_cache: bool = True) -> str:\n",
    "    if use_cache and temperature <= 0.2:\n",
    "        key = _hash_messages(messages, model, temperature, max_tokens)\n",
    "        if key in _llm_cache:\n",
    "            return _llm_cache[key]\n",
    "\n",
    "    delay = BASE_DELAY\n",
    "    last_err = None\n",
    "    consecutive_429 = 0\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                timeout=timeout,\n",
    "                n=1,\n",
    "            )\n",
    "            text = resp.choices[0].message.content\n",
    "            if use_cache and temperature <= 0.2:\n",
    "                _llm_cache[_hash_messages(messages, model, temperature, max_tokens)] = text\n",
    "            return text\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"insufficient_quota\" in msg or \"quota\" in msg:\n",
    "                raise RuntimeError(\"Groq reports insufficient quota for this key.\") from e\n",
    "            consecutive_429 += 1\n",
    "            if consecutive_429 >= CONSECUTIVE_429_ABORT:\n",
    "                raise TooManyRateLimits(\"Aborting stage due to repeated 429s\") from e\n",
    "            print(f\"[429] {consecutive_429}/{CONSECUTIVE_429_ABORT} → cooling {PAUSE_ON_RATE_LIMIT_SEC}s\")\n",
    "            time.sleep(PAUSE_ON_RATE_LIMIT_SEC)\n",
    "            last_err = e\n",
    "\n",
    "        except (APITimeoutError, APIConnectionError, APIError) as e:\n",
    "            msg = str(e).lower()\n",
    "            if any(bad in msg for bad in [\"invalid api key\",\"unsupported model\",\"model_not_found\",\"invalid_request_error\",\"insufficient_quota\"]):\n",
    "                raise\n",
    "            sleep_for = min(BACKOFF_CAP, delay) * (1 + random.random()*0.3)\n",
    "            print(f\"[Retryable] attempt {attempt}/{MAX_RETRIES} → sleeping {sleep_for:.1f}s\")\n",
    "            time.sleep(sleep_for)\n",
    "            delay *= BACKOFF_BASE\n",
    "            last_err = e\n",
    "\n",
    "    raise RuntimeError(f\"chat_complete failed after retries. Last error: {last_err}\")\n",
    "\n",
    "print(\"Groq client ready. MODEL_NAME:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f757662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:20.578125Z",
     "iopub.status.busy": "2025-10-21T01:12:20.578023Z",
     "iopub.status.idle": "2025-10-21T01:12:21.618808Z",
     "shell.execute_reply": "2025-10-21T01:12:21.617511Z"
    },
    "papermill": {
     "duration": 1.043704,
     "end_time": "2025-10-21T01:12:21.619672",
     "exception": false,
     "start_time": "2025-10-21T01:12:20.575968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 'gsm8k', split 'main' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Failed: ValueError: Fast download using 'hf_transfer' is enabled (HF_HUB_ENABLE_HF_TRANSFER=1) but 'hf_transfer' package is not available in your environment. Try `pip install hf_transfer`.\n",
      "Trying 'openai/gsm8k', split 'main' ...\n",
      "  Failed: ValueError: Fast download using 'hf_transfer' is enabled (HF_HUB_ENABLE_HF_TRANSFER=1) but 'hf_transfer' package is not available in your environment. Try `pip install hf_transfer`.\n",
      "Trying 'gsm8k' with streaming=True ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Streaming failed: ValueError: Fast download using 'hf_transfer' is enabled (HF_HUB_ENABLE_HF_TRANSFER=1) but 'hf_transfer' package is not available in your environment. Try `pip install hf_transfer`.\n",
      "Trying 'openai/gsm8k' with streaming=True ...\n",
      "  Streaming failed: ValueError: Fast download using 'hf_transfer' is enabled (HF_HUB_ENABLE_HF_TRANSFER=1) but 'hf_transfer' package is not available in your environment. Try `pip install hf_transfer`.\n",
      "\n",
      "  Could not load GSM8K (network/pipe issue). Using small built-in sample.\n",
      "\n",
      " Saved fresh subset to gsm8k_subset.csv (5 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sarah has 3 apples and buys 4 more. How many a...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A box holds 6 pens. There are 5 boxes. How man...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom read 12 pages on Monday and 13 on Tuesday....</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question gt_answer\n",
       "0  Sarah has 3 apples and buys 4 more. How many a...         7\n",
       "1  A box holds 6 pens. There are 5 boxes. How man...        30\n",
       "2  Tom read 12 pages on Monday and 13 on Tuesday....        25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 2: GSM8K loader with retries/streaming + local fallback\n",
    "import os, re, pandas as pd\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "CSV_PATH = \"gsm8k_subset.csv\"\n",
    "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n",
    "\n",
    "def extract_final_number(text: str) -> Optional[str]:\n",
    "    m = re.findall(r\"####\\s*([+-]?\\d[\\d,]*(?:\\.\\d+)?)\", str(text))\n",
    "    if not m: return None\n",
    "    return m[-1].replace(\",\", \"\").strip()\n",
    "\n",
    "def to_float_str(s: str) -> Optional[str]:\n",
    "    try:\n",
    "        f = float(str(s).strip())\n",
    "        if abs(f) < 1e-12: f = 0.0\n",
    "        if abs(f - round(f)) < 1e-9: return str(int(round(f)))\n",
    "        return str(f)\n",
    "    except: return None\n",
    "\n",
    "def build_df_from_hf_split(split, take: int, seed: int = 42) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    split = split.shuffle(seed=seed).select(range(min(take, len(split))))\n",
    "    for r in split:\n",
    "        q = r[\"question\"]\n",
    "        gt = extract_final_number(r[\"answer\"])\n",
    "        gt_norm = to_float_str(gt) if gt is not None else None\n",
    "        if gt_norm is not None:\n",
    "            rows.append({\"question\": q, \"gt_answer\": gt_norm})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def try_load_gsm8k(max_questions: int, seed: int = 42) -> pd.DataFrame:\n",
    "    candidates = [(\"gsm8k\", \"main\"), (\"openai/gsm8k\", \"main\")]\n",
    "    last_err = None\n",
    "\n",
    "    # 1) normal download (portable DownloadConfig)\n",
    "    for ds_id, name in candidates:\n",
    "        try:\n",
    "            print(f\"Trying '{ds_id}', split '{name}' ...\")\n",
    "            cfg = DownloadConfig(max_retries=5)\n",
    "            ds = load_dataset(ds_id, name, download_config=cfg)\n",
    "            df = build_df_from_hf_split(ds[\"train\"], max_questions, seed=seed)\n",
    "            if len(df) == 0: raise RuntimeError(\"Dataset loaded but parsed 0 numeric rows.\")\n",
    "            print(f\"Loaded {len(df)} items from '{ds_id}'.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed: {type(e).__name__}: {e}\")\n",
    "            last_err = e\n",
    "\n",
    "    # 2) streaming fallback\n",
    "    for ds_id, name in candidates:\n",
    "        try:\n",
    "            print(f\"Trying '{ds_id}' with streaming=True ...\")\n",
    "            ds = load_dataset(ds_id, name, streaming=True)\n",
    "            rows = []\n",
    "            for i, r in enumerate(ds[\"train\"]):\n",
    "                if i >= max_questions: break\n",
    "                q = r[\"question\"]\n",
    "                gt = extract_final_number(r[\"answer\"])\n",
    "                gt_norm = to_float_str(gt) if gt is not None else None\n",
    "                if gt_norm is not None:\n",
    "                    rows.append({\"question\": q, \"gt_answer\": gt_norm})\n",
    "            df = pd.DataFrame(rows)\n",
    "            if len(df) == 0: raise RuntimeError(\"Streaming worked but produced 0 numeric rows.\")\n",
    "            print(f\"Loaded {len(df)} items from '{ds_id}' (streaming).\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  Streaming failed: {type(e).__name__}: {e}\")\n",
    "            last_err = e\n",
    "\n",
    "    # 3) ultimate fallback: small built-in sample\n",
    "    print(\"\\n  Could not load GSM8K (network/pipe issue). Using small built-in sample.\")\n",
    "    local_sample = [\n",
    "        {\"question\": \"Sarah has 3 apples and buys 4 more. How many apples total?\", \"gt_answer\": \"7\"},\n",
    "        {\"question\": \"A box holds 6 pens. There are 5 boxes. How many pens total?\", \"gt_answer\": \"30\"},\n",
    "        {\"question\": \"Tom read 12 pages on Monday and 13 on Tuesday. How many total?\", \"gt_answer\": \"25\"},\n",
    "        {\"question\": \"A train travels 60 miles in 1.5 hours. What is the speed (mph)?\", \"gt_answer\": \"40\"},\n",
    "        {\"question\": \"There are 8 rows of 7 chairs. How many chairs?\", \"gt_answer\": \"56\"},\n",
    "    ]\n",
    "    return pd.DataFrame(local_sample)\n",
    "\n",
    "# knobs\n",
    "import random\n",
    "MAX_QUESTIONS = int(os.getenv(\"MAX_QUESTIONS\", \"300\"))\n",
    "random.seed(42)\n",
    "\n",
    "data_df = try_load_gsm8k(MAX_QUESTIONS, seed=42)\n",
    "data_df[\"gt_answer\"] = data_df[\"gt_answer\"].apply(to_float_str)\n",
    "data_df = data_df.dropna(subset=[\"gt_answer\"]).reset_index(drop=True)\n",
    "data_df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"\\n Saved fresh subset to {CSV_PATH} ({len(data_df)} rows)\")\n",
    "display(data_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d079e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:21.627860Z",
     "iopub.status.busy": "2025-10-21T01:12:21.627594Z",
     "iopub.status.idle": "2025-10-21T01:12:21.648082Z",
     "shell.execute_reply": "2025-10-21T01:12:21.646657Z"
    },
    "papermill": {
     "duration": 0.025109,
     "end_time": "2025-10-21T01:12:21.648628",
     "exception": false,
     "start_time": "2025-10-21T01:12:21.623519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client ready. MODEL_NAME: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "#Step 3\n",
    "import os, time, random, hashlib, json, re\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "\n",
    "PROVIDER   = os.getenv(\"PROVIDER\", \"groq\").lower()\n",
    "BASE_URL   = os.getenv(\"BASE_URL\", \"https://api.groq.com/openai/v1\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama-3.1-8b-instant\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "assert PROVIDER == \"groq\", \"Set PROVIDER=groq in your .env\"\n",
    "assert GROQ_API_KEY, \"Missing GROQ_API_KEY in .env\"\n",
    "assert BASE_URL and MODEL_NAME, \"BASE_URL / MODEL_NAME missing\"\n",
    "\n",
    "_client = OpenAI(api_key=GROQ_API_KEY, base_url=BASE_URL)\n",
    "\n",
    "class TooManyRateLimits(RuntimeError): ...\n",
    "# conservative knobs\n",
    "MAX_RETRIES = 3\n",
    "PAUSE_ON_RATE_LIMIT_SEC = 5\n",
    "BASE_DELAY = 1.0\n",
    "BACKOFF_BASE = 1.6\n",
    "BACKOFF_CAP = 8.0\n",
    "CONSECUTIVE_429_ABORT = 3\n",
    "\n",
    "# low-temp cache\n",
    "_llm_cache = {}\n",
    "def _hash_messages(messages, model, temperature, max_tokens):\n",
    "    payload = {\"m\": messages, \"model\": model, \"t\": temperature, \"max_tokens\": max_tokens}\n",
    "    return hashlib.sha256(json.dumps(payload, sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "def chat_complete(messages: List[Dict], model: str = MODEL_NAME, temperature: float = 0.2,\n",
    "                  max_tokens: int = 512, timeout: int = 60, use_cache: bool = True) -> str:\n",
    "    if use_cache and temperature <= 0.2:\n",
    "        key = _hash_messages(messages, model, temperature, max_tokens)\n",
    "        if key in _llm_cache:\n",
    "            return _llm_cache[key]\n",
    "\n",
    "    delay = BASE_DELAY\n",
    "    last_err = None\n",
    "    consecutive_429 = 0\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                timeout=timeout,\n",
    "                n=1,\n",
    "            )\n",
    "            text = resp.choices[0].message.content\n",
    "            if use_cache and temperature <= 0.2:\n",
    "                _llm_cache[_hash_messages(messages, model, temperature, max_tokens)] = text\n",
    "            return text\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            msg = str(e).lower()\n",
    "            if \"insufficient_quota\" in msg or \"quota\" in msg:\n",
    "                raise RuntimeError(\"Groq reports insufficient quota for this key.\") from e\n",
    "            consecutive_429 += 1\n",
    "            if consecutive_429 >= CONSECUTIVE_429_ABORT:\n",
    "                raise TooManyRateLimits(\"Aborting stage due to repeated 429s\") from e\n",
    "            print(f\"[429] {consecutive_429}/{CONSECUTIVE_429_ABORT} → cooling {PAUSE_ON_RATE_LIMIT_SEC}s\")\n",
    "            time.sleep(PAUSE_ON_RATE_LIMIT_SEC)\n",
    "            last_err = e\n",
    "\n",
    "        except (APITimeoutError, APIConnectionError, APIError) as e:\n",
    "            msg = str(e).lower()\n",
    "            if any(bad in msg for bad in [\"invalid api key\",\"unsupported model\",\"model_not_found\",\"invalid_request_error\",\"insufficient_quota\"]):\n",
    "                raise\n",
    "            sleep_for = min(BACKOFF_CAP, delay) * (1 + random.random()*0.3)\n",
    "            print(f\"[Retryable] attempt {attempt}/{MAX_RETRIES} → sleeping {sleep_for:.1f}s\")\n",
    "            time.sleep(sleep_for)\n",
    "            delay *= BACKOFF_BASE\n",
    "            last_err = e\n",
    "\n",
    "    raise RuntimeError(f\"chat_complete failed after retries. Last error: {last_err}\")\n",
    "\n",
    "print(\"Groq client ready. MODEL_NAME:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd670810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:21.662778Z",
     "iopub.status.busy": "2025-10-21T01:12:21.662580Z",
     "iopub.status.idle": "2025-10-21T01:12:21.668422Z",
     "shell.execute_reply": "2025-10-21T01:12:21.667449Z"
    },
    "papermill": {
     "duration": 0.010385,
     "end_time": "2025-10-21T01:12:21.669159",
     "exception": false,
     "start_time": "2025-10-21T01:12:21.658774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 4: Helpers (parser + prompt builder)\n",
    "def to_float_str(s: str) -> Optional[str]:\n",
    "    try:\n",
    "        f = float(str(s).strip())\n",
    "        if abs(f) < 1e-12: f = 0.0\n",
    "        if abs(f - round(f)) < 1e-9: return str(int(round(f)))\n",
    "        return str(f)\n",
    "    except: return None\n",
    "\n",
    "def parse_pred_answer(text: str) -> Optional[str]:\n",
    "    s = str(text)\n",
    "    m = re.findall(r\"####\\s*([+-]?\\d[\\d,]*(?:\\.\\d+)?)\", s)\n",
    "    if m: return to_float_str(m[-1].replace(\",\", \"\"))\n",
    "    m = re.findall(r\"(?:final answer|answer)\\s*[:\\-]?\\s*([+-]?\\d[\\d,]*(?:\\.\\d+)?)\", s, flags=re.I)\n",
    "    if m: return to_float_str(m[-1].replace(\",\", \"\"))\n",
    "    m = re.findall(r\"([+-]?\\d[\\d,]*(?:\\.\\d+)?)\", s)\n",
    "    if m: return to_float_str(m[-1].replace(\",\", \"\"))\n",
    "    return None\n",
    "\n",
    "SYSTEM_PRIME = \"Follow the user’s instructions exactly. Only produce the format requested.\"\n",
    "\n",
    "def build_query(prompt_header: str, question: str) -> List[Dict]:\n",
    "    content = f\"\"\"{prompt_header}\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "STRICT OUTPUT RULES:\n",
    "- Put only the final numeric answer on a new line prefixed exactly with four hashes and a space.\n",
    "- Example: #### 40\n",
    "- Do not add units or extra text after the number.\n",
    "\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PRIME},\n",
    "        {\"role\": \"user\",   \"content\": content},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d29b999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:21.675739Z",
     "iopub.status.busy": "2025-10-21T01:12:21.675510Z",
     "iopub.status.idle": "2025-10-21T01:12:21.679370Z",
     "shell.execute_reply": "2025-10-21T01:12:21.678637Z"
    },
    "papermill": {
     "duration": 0.007524,
     "end_time": "2025-10-21T01:12:21.679786",
     "exception": false,
     "start_time": "2025-10-21T01:12:21.672262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 5:Prompt templates\n",
    "BASE_PROMPT = \"\"\"You will answer an elementary math word problem.\n",
    "Provide only the final numeric answer at the end on a new line as: #### ANSWER\n",
    "Do not include units in the final answer.\n",
    "\"\"\"\n",
    "\n",
    "MANUAL_IMPROVED_PROMPT = \"\"\"You are an expert math tutor. Think step by step and verify each calculation carefully.\n",
    "Solve the problem using clear reasoning, compute the final value, and double-check arithmetic.\n",
    "At the end, output only the final numeric answer on a new line as: #### ANSWER\n",
    "Do not include units in the final answer.\n",
    "\"\"\"\n",
    "\n",
    "FEW_SHOT_BLOCK = \"\"\"\n",
    "Here are a few examples:\n",
    "\n",
    "Q: Sarah has 3 apples and buys 4 more. How many apples does she have now?\n",
    "Reasoning: 3 + 4 = 7\n",
    "#### 7\n",
    "\n",
    "Q: A box holds 6 pens. If there are 5 boxes, how many pens in total?\n",
    "Reasoning: 6 * 5 = 30\n",
    "#### 30\n",
    "\n",
    "Q: Tom read 12 pages on Monday and 13 on Tuesday. How many pages total?\n",
    "Reasoning: 12 + 13 = 25\n",
    "#### 25\n",
    "\n",
    "Now solve the next problem.\n",
    "\"\"\"\n",
    "\n",
    "FEW_SHOT_PROMPT = f\"\"\"You are an expert math tutor. Think step by step and verify each calculation carefully.\n",
    "{FEW_SHOT_BLOCK}\n",
    "At the end, output only the final numeric answer on a new line as: #### ANSWER\n",
    "Do not include units in the final answer.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f564616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:21.684665Z",
     "iopub.status.busy": "2025-10-21T01:12:21.684508Z",
     "iopub.status.idle": "2025-10-21T01:12:21.693305Z",
     "shell.execute_reply": "2025-10-21T01:12:21.692549Z"
    },
    "papermill": {
     "duration": 0.01182,
     "end_time": "2025-10-21T01:12:21.693921",
     "exception": false,
     "start_time": "2025-10-21T01:12:21.682101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 6:Evaluators\n",
    "TEMPERATURE      = float(os.getenv(\"TEMPERATURE\", \"0.2\"))\n",
    "SC_TEMPERATURE   = float(os.getenv(\"SC_TEMPERATURE\", \"0.7\"))\n",
    "SC_SAMPLES       = int(os.getenv(\"SC_SAMPLES\", \"3\"))\n",
    "RATE_LIMIT_SLEEP = float(os.getenv(\"RATE_LIMIT_SLEEP\", \"1.0\"))\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def score_one(question: str, gt_answer: str, prompt_header: str, \n",
    "              temperature: float = TEMPERATURE, model: str = MODEL_NAME, \n",
    "              sleep_sec: float = RATE_LIMIT_SLEEP) -> Tuple[int, str]:\n",
    "    try:\n",
    "        msgs = build_query(prompt_header, question)\n",
    "        text = chat_complete(msgs, model=model, temperature=temperature)\n",
    "        pred = parse_pred_answer(text)\n",
    "        time.sleep(sleep_sec)\n",
    "        if pred is None:\n",
    "            return 0, text\n",
    "        return int(pred == gt_answer), text\n",
    "    except Exception as e:\n",
    "        time.sleep(sleep_sec)\n",
    "        return 0, f\"[ERROR] {type(e).__name__}: {e}\"\n",
    "\n",
    "def evaluate_dataset(df: pd.DataFrame, prompt_header: str, temperature=TEMPERATURE, \n",
    "                     model=MODEL_NAME, max_eval: Optional[int] = None):\n",
    "    recs = []\n",
    "    total = len(df) if max_eval is None else min(max_eval, len(df))\n",
    "    try:\n",
    "        for i, row in enumerate(df.itertuples(index=False), start=1):\n",
    "            if max_eval is not None and i > max_eval: break\n",
    "            ok, raw = score_one(row.question, row.gt_answer, prompt_header, temperature=temperature, model=model)\n",
    "            recs.append({\"idx\": i, \"question\": row.question, \"gt_answer\": row.gt_answer, \"correct\": ok, \"raw_output\": raw})\n",
    "            if i % 5 == 0:\n",
    "                got = sum(r[\"correct\"] for r in recs)\n",
    "                print(f\"  progress: {i}/{total} • running acc={got/i:.3f}\")\n",
    "    except TooManyRateLimits as e:\n",
    "        print(\" Stage aborted:\", e)\n",
    "\n",
    "    out = pd.DataFrame(recs)\n",
    "    acc = out[\"correct\"].mean() if len(out) else 0.0\n",
    "    return acc, out\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def self_consistency_vote(question: str, prompt_header: str, k: int = SC_SAMPLES, \n",
    "                          temperature: float = SC_TEMPERATURE, model: str = MODEL_NAME):\n",
    "    preds, raws = [], []\n",
    "    for _ in range(k):\n",
    "        msgs = build_query(prompt_header, question)\n",
    "        text = chat_complete(msgs, model=model, temperature=temperature)\n",
    "        raws.append(text)\n",
    "        pred = parse_pred_answer(text)\n",
    "        if pred is not None:\n",
    "            preds.append(pred)\n",
    "        time.sleep(RATE_LIMIT_SLEEP)\n",
    "    if not preds:\n",
    "        return None, raws\n",
    "    vote = Counter(preds).most_common(1)[0][0]\n",
    "    return vote, raws\n",
    "\n",
    "def evaluate_self_consistency(df: pd.DataFrame, prompt_header: str, k=SC_SAMPLES, \n",
    "                              temperature=SC_TEMPERATURE, model=MODEL_NAME, max_eval: Optional[int]=None):\n",
    "    recs = []\n",
    "    total = len(df) if max_eval is None else min(max_eval, len(df))\n",
    "    try:\n",
    "        for i, row in enumerate(df.itertuples(index=False), start=1):\n",
    "            if max_eval is not None and i > max_eval: break\n",
    "            vote, raws = self_consistency_vote(row.question, prompt_header, k=k, temperature=temperature, model=model)\n",
    "            ok = int(vote == row.gt_answer) if vote is not None else 0\n",
    "            recs.append({\"idx\": i, \"question\": row.question, \"gt_answer\": row.gt_answer, \"pred_vote\": vote, \"correct\": ok, \"raw_samples\": raws})\n",
    "            if i % 5 == 0:\n",
    "                got = sum(r[\"correct\"] for r in recs)\n",
    "                print(f\"  SC progress: {i}/{total} • running acc={got/i:.3f}\")\n",
    "    except TooManyRateLimits as e:\n",
    "        print(\" Self-consistency aborted:\", e)\n",
    "\n",
    "    out = pd.DataFrame(recs)\n",
    "    acc = out[\"correct\"].mean() if len(out) else 0.0\n",
    "    return acc, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c490454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:21.698046Z",
     "iopub.status.busy": "2025-10-21T01:12:21.697908Z",
     "iopub.status.idle": "2025-10-21T01:12:43.230916Z",
     "shell.execute_reply": "2025-10-21T01:12:43.229488Z"
    },
    "papermill": {
     "duration": 21.536107,
     "end_time": "2025-10-21T01:12:43.231878",
     "exception": false,
     "start_time": "2025-10-21T01:12:21.695771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Baseline (minimal prompt) ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "Baseline accuracy: 1.000\n",
      "\n",
      "== Manual improved prompt (role + step-by-step) ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "Manual improved accuracy: 1.000\n",
      "\n",
      "== Few-shot prompt (short exemplars + CoT) ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "Few-shot accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "#Step 7:Baseline -> Manual -> Few-shot\n",
    "RESULTS = {}\n",
    "PER_STAGE_LIMIT = int(os.getenv(\"PER_STAGE_LIMIT\", \"5\"))  # try 5–10 first\n",
    "\n",
    "print(\"== Baseline (minimal prompt) ==\")\n",
    "acc_base, df_base = evaluate_dataset(\n",
    "    data_df, BASE_PROMPT, temperature=0.0, model=MODEL_NAME, max_eval=PER_STAGE_LIMIT\n",
    ")\n",
    "print(f\"Baseline accuracy: {acc_base:.3f}\")\n",
    "RESULTS[\"baseline\"] = acc_base\n",
    "df_base.to_csv(\"results_baseline.csv\", index=False)\n",
    "\n",
    "print(\"\\n== Manual improved prompt (role + step-by-step) ==\")\n",
    "acc_manual, df_manual = evaluate_dataset(\n",
    "    data_df, MANUAL_IMPROVED_PROMPT, temperature=0.0, model=MODEL_NAME, max_eval=PER_STAGE_LIMIT\n",
    ")\n",
    "print(f\"Manual improved accuracy: {acc_manual:.3f}\")\n",
    "RESULTS[\"manual_improved\"] = acc_manual\n",
    "df_manual.to_csv(\"results_manual_improved.csv\", index=False)\n",
    "\n",
    "print(\"\\n== Few-shot prompt (short exemplars + CoT) ==\")\n",
    "acc_fewshot, df_few_shot = evaluate_dataset(\n",
    "    data_df, FEW_SHOT_PROMPT, temperature=0.2, model=MODEL_NAME, max_eval=PER_STAGE_LIMIT\n",
    ")\n",
    "print(f\"Few-shot accuracy: {acc_fewshot:.3f}\")\n",
    "RESULTS[\"few_shot\"] = acc_fewshot\n",
    "df_few_shot.to_csv(\"results_few_shot.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8caab34e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:43.243076Z",
     "iopub.status.busy": "2025-10-21T01:12:43.242834Z",
     "iopub.status.idle": "2025-10-21T01:12:55.929333Z",
     "shell.execute_reply": "2025-10-21T01:12:55.927589Z"
    },
    "papermill": {
     "duration": 12.692729,
     "end_time": "2025-10-21T01:12:55.930514",
     "exception": false,
     "start_time": "2025-10-21T01:12:43.237785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Self-consistency over few-shot prompt (k samples, majority vote) ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-consistency accuracy: 1.000\n",
      "\n",
      "== Summary so far ==\n",
      "            baseline: 1.000\n",
      "     manual_improved: 1.000\n",
      "            few_shot: 1.000\n",
      "    self_consistency: 1.000\n"
     ]
    }
   ],
   "source": [
    "#Step 8: Self-consistency\n",
    "print(\"\\n== Self-consistency over few-shot prompt (k samples, majority vote) ==\")\n",
    "SC_SAMPLES = max(3, int(os.getenv(\"SC_SAMPLES\", \"3\")))  # 3 to start; 5–7 later\n",
    "SC_EVAL_LIMIT = int(os.getenv(\"SC_EVAL_LIMIT\", \"3\"))\n",
    "\n",
    "acc_sc, df_sc = evaluate_self_consistency(\n",
    "    data_df, FEW_SHOT_PROMPT, k=SC_SAMPLES, temperature=0.7, model=MODEL_NAME, max_eval=SC_EVAL_LIMIT\n",
    ")\n",
    "print(f\"Self-consistency accuracy: {acc_sc:.3f}\")\n",
    "RESULTS[\"self_consistency\"] = acc_sc\n",
    "df_sc.to_csv(\"results_self_consistency.csv\", index=False)\n",
    "\n",
    "print(\"\\n== Summary so far ==\")\n",
    "for k, v in RESULTS.items():\n",
    "    print(f\"{k:>20}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6adbb61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:12:55.941457Z",
     "iopub.status.busy": "2025-10-21T01:12:55.941217Z",
     "iopub.status.idle": "2025-10-21T01:14:23.720719Z",
     "shell.execute_reply": "2025-10-21T01:14:23.718319Z"
    },
    "papermill": {
     "duration": 87.785643,
     "end_time": "2025-10-21T01:14:23.721702",
     "exception": false,
     "start_time": "2025-10-21T01:12:55.936059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Automated Prompt Search: 1 gen × 2 candidates ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating candidate 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "  -> accuracy (subset 5): 1.000\n",
      "\n",
      "Evaluating candidate 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "  -> accuracy (subset 5): 1.000\n",
      "\n",
      "Evaluating candidate 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "  -> accuracy (subset 5): 1.000\n",
      "\n",
      "Evaluating candidate 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "  -> accuracy (subset 5): 1.000\n",
      "\n",
      "Evaluating candidate 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "  -> accuracy (subset 5): 1.000\n",
      "\n",
      "Best automated instruction block:\n",
      "\n",
      "You will answer an elementary math word problem.\n",
      "Provide only the final numeric answer at the end on a new line as: #### ANSWER\n",
      "Do not include units in the final answer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  progress: 5/5 • running acc=1.000\n",
      "\n",
      "Automated-best (full set) accuracy: 1.000\n",
      "\n",
      "== Final Summary ==\n",
      "                baseline: 1.000\n",
      "         manual_improved: 1.000\n",
      "                few_shot: 1.000\n",
      "        self_consistency: 1.000\n",
      "     automated_best_full: 1.000\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Automated prompt search (OPRO-style) on a subset\n",
    "AUTO_MAX_EVAL = min(50, len(data_df))   # keep small first\n",
    "CANDIDATES_PER_GEN = 2\n",
    "GENERATIONS = 1\n",
    "\n",
    "OPRO_META_PROMPT = \"\"\"You are optimizing the instruction that precedes math questions to improve accuracy.\n",
    "Propose ONE improved instruction block (2-4 sentences) that helps a small model solve grade-school math word problems.\n",
    "Emphasize step-by-step reasoning, estimation checks, and a final numeric answer formatting exactly like '#### ANSWER'.\n",
    "Return ONLY the instruction text, no explanation.\n",
    "\"\"\"\n",
    "\n",
    "def propose_instruction_blocks(n: int) -> List[str]:\n",
    "    props = []\n",
    "    for _ in range(n):\n",
    "        text = chat_complete([{\"role\":\"user\",\"content\": OPRO_META_PROMPT}],\n",
    "                             model=MODEL_NAME, temperature=0.7, max_tokens=250)\n",
    "        if \"#### ANSWER\" not in text:\n",
    "            text = text.strip() + \"\\nAlways end with the final numeric answer line formatted exactly as: #### ANSWER\"\n",
    "        props.append(text.strip())\n",
    "        time.sleep(RATE_LIMIT_SLEEP)\n",
    "    return props\n",
    "\n",
    "def evaluate_instruction_block(instr: str, max_eval=AUTO_MAX_EVAL):\n",
    "    acc, out = evaluate_dataset(data_df, instr, temperature=0.2, model=MODEL_NAME, max_eval=max_eval)\n",
    "    return acc, out\n",
    "\n",
    "pool = [BASE_PROMPT.strip(), MANUAL_IMPROVED_PROMPT.strip(), FEW_SHOT_PROMPT.strip()]\n",
    "history = []\n",
    "\n",
    "print(f\"\\n=== Automated Prompt Search: {GENERATIONS} gen × {CANDIDATES_PER_GEN} candidates ===\")\n",
    "new_instrs = propose_instruction_blocks(CANDIDATES_PER_GEN)\n",
    "pool = pool + new_instrs\n",
    "\n",
    "scored = []\n",
    "for i, instr in enumerate(pool):\n",
    "    print(f\"\\nEvaluating candidate {i+1}/{len(pool)}\")\n",
    "    acc, out = evaluate_instruction_block(instr, max_eval=AUTO_MAX_EVAL)\n",
    "    print(f\"  -> accuracy (subset {AUTO_MAX_EVAL}): {acc:.3f}\")\n",
    "    scored.append((acc, instr))\n",
    "scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "best_auto_prompt = scored[0][1]\n",
    "print(\"\\nBest automated instruction block:\\n\")\n",
    "print(best_auto_prompt)\n",
    "\n",
    "#Final full evaluation of automated-best (still capped by PER_STAGE_LIMIT if set)\n",
    "acc_auto_full, df_auto_full = evaluate_dataset(data_df, best_auto_prompt, temperature=0.2, model=MODEL_NAME)\n",
    "print(f\"\\nAutomated-best (full set) accuracy: {acc_auto_full:.3f}\")\n",
    "RESULTS[\"automated_best_full\"] = acc_auto_full\n",
    "df_auto_full.to_csv(\"results_automated_best_full.csv\", index=False)\n",
    "\n",
    "print(\"\\n== Final Summary ==\")\n",
    "for k, v in RESULTS.items():\n",
    "    print(f\"{k:>24}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36b6a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T01:14:23.734208Z",
     "iopub.status.busy": "2025-10-21T01:14:23.734008Z",
     "iopub.status.idle": "2025-10-21T01:14:23.753942Z",
     "shell.execute_reply": "2025-10-21T01:14:23.752851Z"
    },
    "papermill": {
     "duration": 0.025943,
     "end_time": "2025-10-21T01:14:23.754513",
     "exception": false,
     "start_time": "2025-10-21T01:14:23.728570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manual_improved</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>few_shot</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self_consistency</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>automated_best_full</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt_type  accuracy\n",
       "0             baseline       1.0\n",
       "1      manual_improved       1.0\n",
       "2             few_shot       1.0\n",
       "3     self_consistency       1.0\n",
       "4  automated_best_full       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: summary_accuracies.csv + preview_*.csv\n"
     ]
    }
   ],
   "source": [
    "#Step 10: Save accuracy table + preview CSVs\n",
    "summary_rows = [{\"prompt_type\": name, \"accuracy\": float(acc)} for name, acc in RESULTS.items()]\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"accuracy\", ascending=False)\n",
    "summary_df.to_csv(\"summary_accuracies.csv\", index=False)\n",
    "display(summary_df)\n",
    "\n",
    "def sample_preview(fname: str, df: pd.DataFrame, n=10):\n",
    "    cols = [c for c in [\"idx\",\"question\",\"gt_answer\",\"correct\",\"raw_output\",\"pred_vote\",\"raw_samples\"] if c in df.columns]\n",
    "    df.head(n)[cols].to_csv(fname, index=False)\n",
    "\n",
    "sample_preview(\"preview_baseline.csv\", df_base)\n",
    "sample_preview(\"preview_manual_improved.csv\", df_manual)\n",
    "sample_preview(\"preview_few_shot.csv\", df_few_shot)\n",
    "if 'df_sc' in globals():\n",
    "    df_sc.head(10)[[\"idx\",\"question\",\"gt_answer\",\"pred_vote\",\"correct\"]].to_csv(\"preview_self_consistency.csv\", index=False)\n",
    "if 'df_auto_full' in globals():\n",
    "    df_auto_full.head(10)[[\"idx\",\"question\",\"gt_answer\",\"correct\",\"raw_output\"]].to_csv(\"preview_automated_best.csv\", index=False)\n",
    "\n",
    "print(\"Wrote: summary_accuracies.csv + preview_*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f0846",
   "metadata": {
    "papermill": {
     "duration": 0.002637,
     "end_time": "2025-10-21T01:14:23.760195",
     "exception": false,
     "start_time": "2025-10-21T01:14:23.757558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msai339",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 124.665122,
   "end_time": "2025-10-21T01:14:24.087340",
   "environment_variables": {},
   "exception": null,
   "input_path": "IEMS_490-0_Assignment1.ipynb",
   "output_path": "/outputs/Assignment1_executed.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T01:12:19.422218",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}